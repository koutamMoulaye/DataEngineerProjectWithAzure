[0m13:02:32.643525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC801D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC8DB7D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC8DB67A0>]}


============================== 13:02:32.648598 | 88f1138a-bd67-4c66-9210-8895f84a7a98 ==============================
[0m13:02:32.648598 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:02:32.649502 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:02:32.668505 [info ] [MainThread]: dbt version: 1.9.4
[0m13:02:32.670502 [info ] [MainThread]: python version: 3.10.0
[0m13:02:32.671504 [info ] [MainThread]: python path: C:\Users\mlkou\AppData\Local\Programs\Python\Python310\python.exe
[0m13:02:32.672502 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m13:02:33.349715 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:02:33.350727 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:02:33.350727 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:02:34.145752 [info ] [MainThread]: Using profiles dir at C:\Users\mlkou\.dbt
[0m13:02:34.146749 [info ] [MainThread]: Using profiles.yml file at C:\Users\mlkou\.dbt\profiles.yml
[0m13:02:34.147754 [info ] [MainThread]: Using dbt_project.yml file at Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\dbt_project.yml
[0m13:02:34.148751 [info ] [MainThread]: adapter type: databricks
[0m13:02:34.148751 [info ] [MainThread]: adapter version: 1.9.7
[0m13:02:34.281517 [info ] [MainThread]: Configuration:
[0m13:02:34.282518 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:02:34.283530 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:02:34.284518 [info ] [MainThread]: Required dependencies:
[0m13:02:34.284518 [debug] [MainThread]: Executing "git --help"
[0m13:02:34.316387 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:02:34.316387 [debug] [MainThread]: STDERR: "b''"
[0m13:02:34.317490 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:02:34.318455 [info ] [MainThread]: Connection:
[0m13:02:34.319383 [info ] [MainThread]:   host: adb-1336433203988257.17.azuredatabricks.net
[0m13:02:34.320427 [info ] [MainThread]:   http_path: sql/protocolv1/o/1336433203988257/0405-110946-y0tiniq
[0m13:02:34.321389 [info ] [MainThread]:   catalog: hive_metastore
[0m13:02:34.321389 [info ] [MainThread]:   schema: saleslt
[0m13:02:34.323389 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m13:02:34.755755 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2318648886464, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13600, 14388), compute-name=) - Creating connection
[0m13:02:34.756752 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m13:02:34.756752 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2318648886464, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(13600, 14388), compute-name=) - Acquired connection on thread (13600, 14388), using default compute resource for model 'None'
[0m13:02:34.757754 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2318648886464, session-id=None, name=debug, idle-time=0.001001596450805664s, acquire-count=1, language=None, thread-identifier=(13600, 14388), compute-name=) - Checking idleness
[0m13:02:34.758758 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2318648886464, session-id=None, name=debug, idle-time=0.002006053924560547s, acquire-count=1, language=None, thread-identifier=(13600, 14388), compute-name=) - Retrieving connection
[0m13:02:34.758758 [debug] [MainThread]: Using databricks connection "debug"
[0m13:02:34.759760 [debug] [MainThread]: On debug: select 1 as id
[0m13:02:34.759760 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:02:35.014537 [debug] [MainThread]: Databricks adapter: Connection(session-id=bae4930b-0506-4df5-b837-2b2875350f53) - Created
[0m13:02:35.436703 [debug] [MainThread]: SQL status: OK in 0.680 seconds
[0m13:02:35.438649 [debug] [MainThread]: Databricks adapter: Cursor(session-id=bae4930b-0506-4df5-b837-2b2875350f53, command-id=34f0f766-65b0-48df-9cee-ff0c335a2314) - Closing
[0m13:02:35.438649 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2318648886464, session-id=bae4930b-0506-4df5-b837-2b2875350f53, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(13600, 14388), compute-name=) - Released connection
[0m13:02:35.439652 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:02:35.439652 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:02:35.441647 [debug] [MainThread]: Command `dbt debug` succeeded at 13:02:35.441647 after 5.61 seconds
[0m13:02:35.441647 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:02:35.442646 [debug] [MainThread]: On debug: Close
[0m13:02:35.442646 [debug] [MainThread]: Databricks adapter: Connection(session-id=bae4930b-0506-4df5-b837-2b2875350f53) - Closing
[0m13:02:35.539496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC801D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC8191EA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BC80E4970>]}
[0m13:02:35.540499 [debug] [MainThread]: Flushing usage events
[0m13:02:35.922303 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:29:24.501294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002583CAA1480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002583E811630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002583E8108B0>]}


============================== 13:29:24.505283 | caeeb365-8845-4d3a-8dff-aa2da72d89c4 ==============================
[0m13:29:24.505283 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:29:24.506283 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt snapshot', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:29:25.146993 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:29:25.146993 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:29:25.148045 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:29:26.217498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'caeeb365-8845-4d3a-8dff-aa2da72d89c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584FD4A7A0>]}
[0m13:29:26.293111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'caeeb365-8845-4d3a-8dff-aa2da72d89c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002583E1C7310>]}
[0m13:29:26.294126 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m13:29:26.924395 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:29:26.926029 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:29:26.926553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'caeeb365-8845-4d3a-8dff-aa2da72d89c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584FE293F0>]}
[0m13:29:30.894784 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.medallion_dbt_spark.address_snapshot' (snapshots\address.sql) depends on a source named 'saleslt.address' which was not found
[0m13:29:30.895717 [debug] [MainThread]: Command `dbt snapshot` failed at 13:29:30.895717 after 6.62 seconds
[0m13:29:30.895717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025851595870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025851507550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258515074C0>]}
[0m13:29:30.897304 [debug] [MainThread]: Flushing usage events
[0m13:29:31.282470 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:47.599485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024645A71420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000246477E4DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000246477E7C40>]}


============================== 13:32:47.604004 | 9553bcd8-bde5-4b92-bb03-3fe6ae5ab037 ==============================
[0m13:32:47.604004 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:32:47.605004 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:32:47.634334 [info ] [MainThread]: dbt version: 1.9.4
[0m13:32:47.634334 [info ] [MainThread]: python version: 3.10.0
[0m13:32:47.635316 [info ] [MainThread]: python path: C:\Users\mlkou\AppData\Local\Programs\Python\Python310\python.exe
[0m13:32:47.636101 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m13:32:48.238510 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:32:48.238510 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:32:48.239566 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:32:48.987778 [info ] [MainThread]: Using profiles dir at C:\Users\mlkou\.dbt
[0m13:32:48.988573 [info ] [MainThread]: Using profiles.yml file at C:\Users\mlkou\.dbt\profiles.yml
[0m13:32:48.988573 [info ] [MainThread]: Using dbt_project.yml file at Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\dbt_project.yml
[0m13:32:48.989723 [info ] [MainThread]: adapter type: databricks
[0m13:32:48.989723 [info ] [MainThread]: adapter version: 1.9.7
[0m13:32:49.105164 [info ] [MainThread]: Configuration:
[0m13:32:49.106160 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:32:49.107078 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:32:49.107078 [info ] [MainThread]: Required dependencies:
[0m13:32:49.108384 [debug] [MainThread]: Executing "git --help"
[0m13:32:49.133518 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:32:49.134619 [debug] [MainThread]: STDERR: "b''"
[0m13:32:49.134619 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:32:49.135604 [info ] [MainThread]: Connection:
[0m13:32:49.136522 [info ] [MainThread]:   host: adb-1336433203988257.17.azuredatabricks.net
[0m13:32:49.137386 [info ] [MainThread]:   http_path: sql/protocolv1/o/1336433203988257/0405-110946-y0tiniq
[0m13:32:49.137871 [info ] [MainThread]:   catalog: hive_metastore
[0m13:32:49.137871 [info ] [MainThread]:   schema: saleslt
[0m13:32:49.138879 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m13:32:49.534314 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2501162631856, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(17812, 17280), compute-name=) - Creating connection
[0m13:32:49.535517 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m13:32:49.536091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2501162631856, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(17812, 17280), compute-name=) - Acquired connection on thread (17812, 17280), using default compute resource for model 'None'
[0m13:32:49.536091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2501162631856, session-id=None, name=debug, idle-time=0.0005736351013183594s, acquire-count=1, language=None, thread-identifier=(17812, 17280), compute-name=) - Checking idleness
[0m13:32:49.536091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2501162631856, session-id=None, name=debug, idle-time=0.0005736351013183594s, acquire-count=1, language=None, thread-identifier=(17812, 17280), compute-name=) - Retrieving connection
[0m13:32:49.536091 [debug] [MainThread]: Using databricks connection "debug"
[0m13:32:49.537181 [debug] [MainThread]: On debug: select 1 as id
[0m13:32:49.537181 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:32:50.199829 [debug] [MainThread]: Databricks adapter: Connection(session-id=67f59a2f-d1a9-4f71-88e7-be4f3d815fac) - Created
[0m13:32:50.698950 [debug] [MainThread]: SQL status: OK in 1.160 seconds
[0m13:32:50.700951 [debug] [MainThread]: Databricks adapter: Cursor(session-id=67f59a2f-d1a9-4f71-88e7-be4f3d815fac, command-id=4da10f5a-3115-4c7e-a1ab-ae86cd31a652) - Closing
[0m13:32:50.700951 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2501162631856, session-id=67f59a2f-d1a9-4f71-88e7-be4f3d815fac, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(17812, 17280), compute-name=) - Released connection
[0m13:32:50.700951 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:32:50.701951 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:32:50.702951 [debug] [MainThread]: Command `dbt debug` succeeded at 13:32:50.702951 after 3.20 seconds
[0m13:32:50.703949 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:32:50.703949 [debug] [MainThread]: On debug: Close
[0m13:32:50.703949 [debug] [MainThread]: Databricks adapter: Connection(session-id=67f59a2f-d1a9-4f71-88e7-be4f3d815fac) - Closing
[0m13:32:50.922329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024645A71420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024658D9D690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024658E5E710>]}
[0m13:32:50.924405 [debug] [MainThread]: Flushing usage events
[0m13:32:51.308232 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:33:15.516255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1ECFA1480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1EED11630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1EED108B0>]}


============================== 13:33:15.521318 | 3a228c7c-cc90-4877-a593-17f36c721ab4 ==============================
[0m13:33:15.521318 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:33:15.522329 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt snapshot', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:33:16.277015 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:33:16.277015 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:33:16.278014 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:33:17.313700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D18038A7A0>]}
[0m13:33:17.394537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1EE6C3310>]}
[0m13:33:17.395534 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m13:33:17.825707 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:33:17.826707 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:33:17.827664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D180468EB0>]}
[0m13:33:20.240219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181EE1240>]}
[0m13:33:20.379936 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m13:33:20.384016 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m13:33:20.420916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181EF59C0>]}
[0m13:33:20.422117 [info ] [MainThread]: Found 2 models, 7 snapshots, 4 data tests, 9 sources, 607 macros
[0m13:33:20.423114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181F7F580>]}
[0m13:33:20.426581 [info ] [MainThread]: 
[0m13:33:20.427601 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:33:20.428531 [info ] [MainThread]: 
[0m13:33:20.428531 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14632, 11944), compute-name=) - Creating connection
[0m13:33:20.430553 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:33:20.430553 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Acquired connection on thread (14632, 11944), using default compute resource for model 'None'
[0m13:33:20.440736 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14632, 17180), compute-name=) - Creating connection
[0m13:33:20.441748 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:33:20.441748 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Acquired connection on thread (14632, 17180), using default compute resource for model 'None'
[0m13:33:20.441748 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Checking idleness
[0m13:33:20.442828 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=None, name=list_hive_metastore, idle-time=0.0010805130004882812s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Retrieving connection
[0m13:33:20.442828 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:33:20.442828 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:33:20.443743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:20.867496 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c) - Created
[0m13:33:21.805205 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m13:33:21.808212 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, command-id=8faae137-ec45-45e8-9fe4-eb80f3377242) - Closing
[0m13:33:21.809779 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 17180), compute-name=) - Released connection
[0m13:33:21.811976 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=list_hive_metastore, idle-time=0.002587556838989258s, acquire-count=0, language=None, thread-identifier=(14632, 17180), compute-name=) - Checking idleness
[0m13:33:21.813368 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m13:33:21.814383 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.006171226501464844s, acquire-count=0, language=None, thread-identifier=(14632, 17180), compute-name=) - Reusing connection previously named list_hive_metastore
[0m13:33:21.816425 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.007173299789428711s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Acquired connection on thread (14632, 17180), using default compute resource for model 'None'
[0m13:33:21.817442 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.009230852127075195s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Checking idleness
[0m13:33:21.818396 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.010184526443481445s, acquire-count=2, language=None, thread-identifier=(14632, 17180), compute-name=) - Acquired connection on thread (14632, 17180), using default compute resource for model 'None'
[0m13:33:21.819977 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m13:33:21.838013 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.029801368713378906s, acquire-count=2, language=None, thread-identifier=(14632, 17180), compute-name=) - Checking idleness
[0m13:33:21.839006 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.030794143676757812s, acquire-count=2, language=None, thread-identifier=(14632, 17180), compute-name=) - Retrieving connection
[0m13:33:21.839006 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.030794143676757812s, acquire-count=2, language=None, thread-identifier=(14632, 17180), compute-name=) - Checking idleness
[0m13:33:21.839006 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.030794143676757812s, acquire-count=2, language=None, thread-identifier=(14632, 17180), compute-name=) - Retrieving connection
[0m13:33:21.840004 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:33:21.840004 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m13:33:21.840509 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m13:33:22.349144 [debug] [ThreadPool]: SQL status: OK in 0.510 seconds
[0m13:33:22.350735 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, command-id=966a4905-52b5-4fb4-b5e1-5651a539beb5) - Closing
[0m13:33:22.350735 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:33:22.351835 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.5436239242553711s, acquire-count=1, language=None, thread-identifier=(14632, 17180), compute-name=) - Released connection
[0m13:33:22.352442 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999339916160, session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c, name=create_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 17180), compute-name=) - Released connection
[0m13:33:22.354550 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14632, 952), compute-name=) - Creating connection
[0m13:33:22.354550 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:33:22.355638 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Acquired connection on thread (14632, 952), using default compute resource for model 'None'
[0m13:33:22.355638 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:22.355638 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Retrieving connection
[0m13:33:22.356636 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:33:22.356636 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt)
[0m13:33:22.356636 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:22.557330 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8) - Created
[0m13:33:23.153381 [debug] [ThreadPool]: SQL status: OK in 0.800 seconds
[0m13:33:23.154460 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, command-id=489d7a5b-e05a-4bc5-b12c-a8a31f9f3412) - Closing
[0m13:33:23.159917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.6000335216522217s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:23.159917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.6015560626983643s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Retrieving connection
[0m13:33:23.159917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.6015560626983643s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:23.160931 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.6025702953338623s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Retrieving connection
[0m13:33:23.160931 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:33:23.160931 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:33:23.160931 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:33:23.430884 [debug] [ThreadPool]: SQL status: OK in 0.270 seconds
[0m13:33:23.465490 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, command-id=e901ee0e-a259-42a5-9f5f-b694269050d1) - Closing
[0m13:33:23.471566 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.9132051467895508s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:23.472583 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.914222240447998s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Retrieving connection
[0m13:33:23.472583 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:33:23.472583 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:33:24.370096 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m13:33:24.374615 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, command-id=2c55dfa6-2227-4de5-b0bc-e3dabdff6212) - Closing
[0m13:33:24.374615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 952), compute-name=) - Released connection
[0m13:33:24.375615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_saleslt, idle-time=0.0009992122650146484s, acquire-count=0, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:24.375615 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:33:24.376614 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_snapshots, idle-time=0.0019986629486083984s, acquire-count=0, language=None, thread-identifier=(14632, 952), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m13:33:24.376614 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_snapshots, idle-time=0.0019986629486083984s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Acquired connection on thread (14632, 952), using default compute resource for model 'None'
[0m13:33:24.376614 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_snapshots, idle-time=0.0019986629486083984s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Checking idleness
[0m13:33:24.377615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_snapshots, idle-time=0.0029993057250976562s, acquire-count=1, language=None, thread-identifier=(14632, 952), compute-name=) - Retrieving connection
[0m13:33:24.377615 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:33:24.380646 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m13:33:24.737264 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m13:33:24.738264 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, command-id=70b1cda7-ba2a-4c6d-ae08-bc835f3d9969) - Closing
[0m13:33:24.738264 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1999341273920, session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 952), compute-name=) - Released connection
[0m13:33:24.739770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181D45720>]}
[0m13:33:24.740851 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=4.310297727584839s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Checking idleness
[0m13:33:24.740851 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=4.310297727584839s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Retrieving connection
[0m13:33:24.741870 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=4.311316967010498s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Checking idleness
[0m13:33:24.741870 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=4.311316967010498s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Retrieving connection
[0m13:33:24.742452 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:33:24.742452 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:33:24.742452 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 11944), compute-name=) - Released connection
[0m13:33:24.751878 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m13:33:24.751878 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m13:33:24.752988 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14632, 22356), compute-name=) - Creating connection
[0m13:33:24.752988 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m13:33:24.754098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m13:33:24.754098 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m13:33:24.761607 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m13:33:24.870779 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.11668133735656738s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:24.871839 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.11774134635925293s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:24.872184 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m13:33:24.872184 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:24.872184 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:33:25.136159 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc) - Created
[0m13:33:26.250744 [debug] [Thread-1 (]: SQL status: OK in 1.380 seconds
[0m13:33:26.250744 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=b162a127-6f6d-45ae-9aaa-6d49e61e8da5) - Closing
[0m13:33:26.254840 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.1186814308166504s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:26.255839 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.1196811199188232s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:26.255839 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m13:33:26.255839 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:26.388411 [debug] [Thread-1 (]: SQL status: OK in 0.130 seconds
[0m13:33:26.388411 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=a56a30bc-f2e1-454c-b16f-634177e78078) - Closing
[0m13:33:26.390931 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.address_snapshot"
[0m13:33:26.491532 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.3543627262115479s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:26.492540 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.3563816547393799s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:26.493653 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.3574941158294678s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:26.493653 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=1.3574941158294678s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:26.494645 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:33:26.494645 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m13:33:26.494645 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
  
[0m13:33:43.136609 [debug] [Thread-1 (]: SQL status: OK in 16.640 seconds
[0m13:33:43.137608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=c5fe70b9-3871-49e4-978c-c83ce9fa2b49) - Closing
[0m13:33:43.588139 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:33:43.596761 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:43.597846 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:43.599813 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1EB49C400>]}
[0m13:33:43.601013 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 18.84s]
[0m13:33:43.603255 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m13:33:43.604227 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m13:33:43.605178 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m13:33:43.606157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.009395360946655273s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:43.606157 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m13:33:43.607217 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.010455131530761719s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.address_snapshot
[0m13:33:43.607217 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.010455131530761719s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m13:33:43.607217 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m13:33:43.612243 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m13:33:43.620792 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.02351236343383789s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:43.621828 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.025066375732421875s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:43.622909 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m13:33:43.623887 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:44.195900 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m13:33:44.196898 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=208f658f-fd07-431f-970b-7a97d6dfd9c8) - Closing
[0m13:33:44.198897 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.602135419845581s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:44.199898 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.6031370162963867s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:44.200401 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m13:33:44.200401 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:44.341953 [debug] [Thread-1 (]: SQL status: OK in 0.140 seconds
[0m13:33:44.342960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=3f5dcabf-4a74-4f44-bb4f-45e9f85bb28e) - Closing
[0m13:33:44.342960 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customer_snapshot"
[0m13:33:44.344951 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.7472641468048096s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:44.346068 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.7481899261474609s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:44.346068 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m13:33:44.347076 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
  
[0m13:33:50.597323 [debug] [Thread-1 (]: SQL status: OK in 6.250 seconds
[0m13:33:50.598348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=7cca2f26-2493-4e83-9f1a-565623b887da) - Closing
[0m13:33:50.708866 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:33:50.709934 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:50.710939 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:50.711613 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181D463B0>]}
[0m13:33:50.712688 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 7.10s]
[0m13:33:50.714305 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m13:33:50.715455 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m13:33:50.716562 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m13:33:50.717095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.007161378860473633s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:50.717619 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m13:33:50.718157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.008223295211791992s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customer_snapshot
[0m13:33:50.718695 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.008761167526245117s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m13:33:50.718695 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m13:33:50.722883 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m13:33:50.730337 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.020403146743774414s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:50.731067 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0211336612701416s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:50.731573 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m13:33:50.732182 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:51.140117 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m13:33:51.140648 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=7828d303-9114-4dca-be34-08e5f4b79b61) - Closing
[0m13:33:51.141671 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.43173742294311523s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:51.142723 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.4327890872955322s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:51.142723 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m13:33:51.142723 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:51.263589 [debug] [Thread-1 (]: SQL status: OK in 0.120 seconds
[0m13:33:51.264593 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=a0353a98-1991-48b8-a670-7316cb7fc86b) - Closing
[0m13:33:51.265601 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m13:33:51.267603 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.557668924331665s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:51.268641 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.5587069988250732s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:51.270321 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m13:33:51.270879 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
  
[0m13:33:55.845990 [debug] [Thread-1 (]: SQL status: OK in 4.580 seconds
[0m13:33:55.847981 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=d6851286-1452-47a5-9942-32db7fd936f0) - Closing
[0m13:33:55.849957 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:33:55.850964 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:55.852414 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:33:55.853420 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D18214CD90>]}
[0m13:33:55.854440 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.14s]
[0m13:33:55.855447 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m13:33:55.856460 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m13:33:55.857458 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m13:33:55.858450 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.006035566329956055s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:55.858450 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m13:33:55.859449 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0070340633392333984s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m13:33:55.859449 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0070340633392333984s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m13:33:55.859449 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m13:33:55.864446 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m13:33:55.874446 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.022031545639038086s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:55.875534 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.022031545639038086s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:55.875534 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m13:33:55.876502 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:56.508043 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m13:33:56.508043 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=3a25af1a-5276-4c91-b870-cab8edb00c88) - Closing
[0m13:33:56.510043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.6566288471221924s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:56.510043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.657628059387207s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:56.510043 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m13:33:56.511043 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:33:56.635395 [debug] [Thread-1 (]: SQL status: OK in 0.120 seconds
[0m13:33:56.635395 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=7647fc4d-2373-4a0c-b829-aac4c700dadd) - Closing
[0m13:33:56.636390 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.product_snapshot"
[0m13:33:56.638389 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.784975528717041s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:33:56.638389 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.7859740257263184s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:33:56.639391 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m13:33:56.640393 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
  
[0m13:34:01.399189 [debug] [Thread-1 (]: SQL status: OK in 4.760 seconds
[0m13:34:01.400189 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=f580fe15-07e5-4cf8-93b4-ebe650215afe) - Closing
[0m13:34:01.402189 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:34:01.403188 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:01.404190 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:01.404190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1EB4B0F70>]}
[0m13:34:01.405189 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 5.55s]
[0m13:34:01.492949 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m13:34:01.492949 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m13:34:01.493964 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m13:34:01.494950 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.09077596664428711s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:01.494950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m13:34:01.495970 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.09278178215026855s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.product_snapshot
[0m13:34:01.495970 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.09278178215026855s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m13:34:01.496969 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m13:34:01.499965 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m13:34:01.505966 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.10277771949768066s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:01.506964 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.1037757396697998s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:01.506964 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m13:34:01.506964 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:01.950087 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m13:34:01.951078 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=89cff6d2-fe94-420c-ae33-0098187087e2) - Closing
[0m13:34:01.953081 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.5489022731781006s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:01.953081 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.5498926639556885s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:01.953081 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m13:34:01.954089 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:02.072819 [debug] [Thread-1 (]: SQL status: OK in 0.120 seconds
[0m13:34:02.072819 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=fa03a66d-b0da-4d76-856a-832a705caf68) - Closing
[0m13:34:02.073787 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m13:34:02.074896 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.6717069149017334s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:02.075794 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.6717069149017334s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:02.075794 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m13:34:02.076897 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
  
[0m13:34:07.447405 [debug] [Thread-1 (]: SQL status: OK in 5.370 seconds
[0m13:34:07.450094 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=6d18c2c2-f568-4082-90de-fc8edf2d037e) - Closing
[0m13:34:07.542356 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:34:07.544354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:07.544354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:07.545354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181EA4310>]}
[0m13:34:07.546354 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 6.05s]
[0m13:34:07.547358 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m13:34:07.547358 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m13:34:07.548866 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m13:34:07.549872 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0055179595947265625s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:07.549872 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m13:34:07.550872 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.006517887115478516s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.productmodel_snapshot
[0m13:34:07.550872 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.006517887115478516s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m13:34:07.551871 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m13:34:07.554871 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m13:34:07.560871 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.015516042709350586s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:07.560871 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.016517162322998047s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:07.560871 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m13:34:07.561872 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:08.110607 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m13:34:08.111524 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=9db98484-d3db-46ec-9fc1-161d50085ee5) - Closing
[0m13:34:08.112534 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.5681805610656738s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:08.113603 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.5692496299743652s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:08.113603 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m13:34:08.114600 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:08.220494 [debug] [Thread-1 (]: SQL status: OK in 0.110 seconds
[0m13:34:08.221492 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=e284678e-aba6-4c32-91a7-0546f4ed405d) - Closing
[0m13:34:08.222500 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m13:34:08.224502 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.6801481246948242s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:08.224502 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.6801481246948242s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:08.225500 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m13:34:08.226500 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
  
[0m13:34:12.651835 [debug] [Thread-1 (]: SQL status: OK in 4.430 seconds
[0m13:34:12.653343 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=b31171d3-fb5e-4507-9af1-0770af271d80) - Closing
[0m13:34:12.654354 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:34:12.655354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:12.656353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:12.656353 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181C0CA60>]}
[0m13:34:12.657357 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 5.11s]
[0m13:34:12.658357 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m13:34:12.658357 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m13:34:12.659353 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m13:34:12.660354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0029997825622558594s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:12.660354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m13:34:12.660354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.004000425338745117s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m13:34:12.661354 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.005000591278076172s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Acquired connection on thread (14632, 22356), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m13:34:12.662367 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m13:34:12.666352 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m13:34:12.671350 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.014997005462646484s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:12.672353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.015999317169189453s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:12.672353 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m13:34:12.673352 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:13.136088 [debug] [Thread-1 (]: SQL status: OK in 0.460 seconds
[0m13:34:13.137172 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=4129dec6-037a-4f69-b64e-e9925ce542ac) - Closing
[0m13:34:13.138179 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.4818258285522461s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:13.139286 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.48293232917785645s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:13.139286 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m13:34:13.139286 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m13:34:13.277742 [debug] [Thread-1 (]: SQL status: OK in 0.140 seconds
[0m13:34:13.278743 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=f00e8660-b0c2-4ada-9fa1-6f98e522ffbe) - Closing
[0m13:34:13.279741 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m13:34:13.281764 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.6254103183746338s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Checking idleness
[0m13:34:13.282747 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.6263933181762695s, acquire-count=1, language=sql, thread-identifier=(14632, 22356), compute-name=) - Retrieving connection
[0m13:34:13.283752 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m13:34:13.283752 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
  
[0m13:34:17.817321 [debug] [Thread-1 (]: SQL status: OK in 4.530 seconds
[0m13:34:17.819326 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, command-id=d46298df-25a1-487f-a235-297e3322acd3) - Closing
[0m13:34:17.821319 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:34:17.822319 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:17.823283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1999341664624, session-id=a7a9c731-566f-4d57-898b-d337fc73adbc, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(14632, 22356), compute-name=) - Released connection
[0m13:34:17.823283 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a228c7c-cc90-4877-a593-17f36c721ab4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181B0F130>]}
[0m13:34:17.824593 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 5.16s]
[0m13:34:17.825593 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m13:34:17.829178 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=53.08672571182251s, acquire-count=0, language=None, thread-identifier=(14632, 11944), compute-name=) - Checking idleness
[0m13:34:17.830203 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=53.08775091171265s, acquire-count=0, language=None, thread-identifier=(14632, 11944), compute-name=) - Reusing connection previously named master
[0m13:34:17.830203 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=53.08775091171265s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Acquired connection on thread (14632, 11944), using default compute resource for model 'None'
[0m13:34:17.831176 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=53.08872437477112s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Checking idleness
[0m13:34:17.831176 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=None, name=master, idle-time=53.08872437477112s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Retrieving connection
[0m13:34:17.831176 [debug] [MainThread]: On master: ROLLBACK
[0m13:34:17.831176 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:34:18.001061 [debug] [MainThread]: Databricks adapter: Connection(session-id=94505321-62a3-48f7-99c0-2937581529af) - Created
[0m13:34:18.001061 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:34:18.001985 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=94505321-62a3-48f7-99c0-2937581529af, name=master, idle-time=0.0009233951568603516s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Checking idleness
[0m13:34:18.001985 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=94505321-62a3-48f7-99c0-2937581529af, name=master, idle-time=0.0009233951568603516s, acquire-count=1, language=None, thread-identifier=(14632, 11944), compute-name=) - Retrieving connection
[0m13:34:18.002982 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:34:18.002982 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:34:18.004081 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1999339737184, session-id=94505321-62a3-48f7-99c0-2937581529af, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14632, 11944), compute-name=) - Released connection
[0m13:34:18.004081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:34:18.004081 [debug] [MainThread]: On master: ROLLBACK
[0m13:34:18.005069 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:34:18.005069 [debug] [MainThread]: On master: Close
[0m13:34:18.005069 [debug] [MainThread]: Databricks adapter: Connection(session-id=94505321-62a3-48f7-99c0-2937581529af) - Closing
[0m13:34:18.090923 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m13:34:18.091945 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m13:34:18.093845 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:34:18.093845 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m13:34:18.094851 [debug] [MainThread]: Databricks adapter: Connection(session-id=d8150ced-95c3-4030-9368-e0c7cab3dc7c) - Closing
[0m13:34:18.200739 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:34:18.200739 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:34:18.201702 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:34:18.201702 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:34:18.201702 [debug] [MainThread]: Databricks adapter: Connection(session-id=75e754dd-7b45-43d2-ad1d-1a1c76208ba8) - Closing
[0m13:34:18.271804 [debug] [MainThread]: Connection 'snapshot.medallion_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m13:34:18.272805 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: ROLLBACK
[0m13:34:18.273782 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:34:18.274801 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: Close
[0m13:34:18.275798 [debug] [MainThread]: Databricks adapter: Connection(session-id=a7a9c731-566f-4d57-898b-d337fc73adbc) - Closing
[0m13:34:18.355217 [info ] [MainThread]: 
[0m13:34:18.356128 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 57.93 seconds (57.93s).
[0m13:34:18.359203 [debug] [MainThread]: Command end result
[0m13:34:18.392638 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m13:34:18.397639 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m13:34:18.406637 [debug] [MainThread]: Wrote artifact RunExecutionResult to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\run_results.json
[0m13:34:18.407639 [info ] [MainThread]: 
[0m13:34:18.408374 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:34:18.409383 [info ] [MainThread]: 
[0m13:34:18.409383 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m13:34:18.412390 [debug] [MainThread]: Command `dbt snapshot` succeeded at 13:34:18.411474 after 63.01 seconds
[0m13:34:18.413414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1ECFA1480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181C632E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D181C63850>]}
[0m13:34:18.414381 [debug] [MainThread]: Flushing usage events
[0m13:34:18.886304 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:39:28.076063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244DA6ED480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244DBDD5780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244DBDD62F0>]}


============================== 22:39:30.761667 | 2f0c72f1-8a50-4095-ad04-bf638245b0c8 ==============================
[0m22:39:30.761667 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:39:30.763747 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:39:34.593485 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:39:34.593485 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:39:34.594483 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:39:38.035594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f0c72f1-8a50-4095-ad04-bf638245b0c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244DC463160>]}
[0m22:39:38.108011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f0c72f1-8a50-4095-ad04-bf638245b0c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244ED6E6A10>]}
[0m22:39:38.109088 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m22:39:38.731858 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:39:41.182752 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 7 files added, 0 files changed.
[0m22:39:41.182752 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\sales.yml
[0m22:39:41.183837 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.sql
[0m22:39:41.183837 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m22:39:41.183837 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m22:39:41.184855 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\sales.sql
[0m22:39:41.184855 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\staging\bronze.yml
[0m22:39:41.184855 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.yml
[0m22:39:41.185837 [debug] [MainThread]: Partial parsing: deleted file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m22:39:41.186837 [debug] [MainThread]: Partial parsing: deleted file: medallion_dbt_spark://models\example\my_second_dbt_model.sql
[0m22:39:41.904681 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m22:39:42.086930 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m22:39:42.202137 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m22:39:42.220941 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.221949 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.221949 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.223231 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.223552 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.224560 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.224560 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.225575 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.225575 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.227622 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.228630 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.229645 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.230796 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.232805 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.234807 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.235804 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.236813 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.237935 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.238943 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:39:42.238943 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:39:42.239962 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:39:42.240941 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:39:42.240941 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:39:42.241943 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:39:42.243034 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:39:42.243853 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:39:42.244412 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:39:42.327096 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.medallion_dbt_spark.example
[0m22:39:42.349769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f0c72f1-8a50-4095-ad04-bf638245b0c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244EF4EA770>]}
[0m22:39:42.471711 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:39:42.489525 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:39:42.581655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f0c72f1-8a50-4095-ad04-bf638245b0c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244EEFB42E0>]}
[0m22:39:42.582943 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 607 macros
[0m22:39:42.583944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f0c72f1-8a50-4095-ad04-bf638245b0c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244EF2C0430>]}
[0m22:39:42.585950 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:39:42.587665 [debug] [MainThread]: Command end result
[0m22:39:42.627699 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:39:42.632304 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:39:42.637816 [debug] [MainThread]: Wrote artifact RunExecutionResult to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\run_results.json
[0m22:39:42.640329 [debug] [MainThread]: Command `dbt test` succeeded at 22:39:42.640329 after 14.95 seconds
[0m22:39:42.641328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244DA6ED480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244EEFD11B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244EF342DA0>]}
[0m22:39:42.642328 [debug] [MainThread]: Flushing usage events
[0m22:39:43.119890 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:42:23.014618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EED22D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEEFA4520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEEFA56C0>]}


============================== 22:42:23.017621 | d9021dde-2aca-4e32-8391-a954cd69b4ea ==============================
[0m22:42:23.017621 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:42:23.019732 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:42:23.661226 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:42:23.662250 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:42:23.662250 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:42:24.632166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEB6DD090>]}
[0m22:42:24.707372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEE957310>]}
[0m22:42:24.707372 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m22:42:25.070013 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:42:25.233760 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:42:25.234833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEE914C10>]}
[0m22:42:27.356461 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m22:42:27.584311 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m22:42:27.638935 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m22:42:27.661078 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:42:27.662199 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:42:27.663218 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:42:27.663218 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m22:42:27.665237 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.666245 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.666245 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.667244 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.668246 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.669247 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.670243 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.670243 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.671272 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.672249 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.672249 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.673244 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.673244 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.675285 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.676292 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.677303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.678300 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.679301 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.680299 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m22:42:27.680804 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:42:27.681810 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:42:27.682810 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:42:27.683811 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m22:42:27.830237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E82D00550>]}
[0m22:42:27.985674 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:42:27.987658 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:42:28.031429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E82CE6170>]}
[0m22:42:28.032427 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 607 macros
[0m22:42:28.032427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEC9365C0>]}
[0m22:42:28.034426 [info ] [MainThread]: 
[0m22:42:28.035428 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:42:28.035428 [info ] [MainThread]: 
[0m22:42:28.036426 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8284, 9900), compute-name=) - Creating connection
[0m22:42:28.036426 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:42:28.037426 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Acquired connection on thread (8284, 9900), using default compute resource for model 'None'
[0m22:42:28.048287 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664247504, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8284, 5316), compute-name=) - Creating connection
[0m22:42:28.049289 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m22:42:28.049831 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664247504, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 5316), compute-name=) - Acquired connection on thread (8284, 5316), using default compute resource for model 'None'
[0m22:42:28.049831 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664247504, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 5316), compute-name=) - Checking idleness
[0m22:42:28.050836 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664247504, session-id=None, name=list_hive_metastore, idle-time=0.0010051727294921875s, acquire-count=1, language=None, thread-identifier=(8284, 5316), compute-name=) - Retrieving connection
[0m22:42:28.050836 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:42:28.050836 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:42:28.050836 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:42:29.040657 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ea2c06ec-353d-45b5-a42f-7629e43fb1a1) - Created
[0m22:42:36.930877 [debug] [ThreadPool]: SQL status: OK in 8.880 seconds
[0m22:42:36.931903 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ea2c06ec-353d-45b5-a42f-7629e43fb1a1, command-id=c46a2070-88cb-43ba-af11-e610875411d9) - Closing
[0m22:42:36.932894 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664247504, session-id=ea2c06ec-353d-45b5-a42f-7629e43fb1a1, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8284, 5316), compute-name=) - Released connection
[0m22:42:36.934902 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8284, 28124), compute-name=) - Creating connection
[0m22:42:36.934902 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m22:42:36.935894 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Acquired connection on thread (8284, 28124), using default compute resource for model 'None'
[0m22:42:36.935894 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:42:36.935894 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:42:36.936884 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:42:36.936884 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt)
[0m22:42:36.936884 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:42:37.228933 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=dd6bc613-49df-4e2e-b433-58905961d735) - Created
[0m22:42:37.803919 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m22:42:37.805922 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=6c0f3928-d04e-4695-a717-47fea172ac9b) - Closing
[0m22:42:37.817479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.5875387191772461s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:42:37.817479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.5875387191772461s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:42:37.818479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.5885388851165771s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:42:37.818983 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.5890426635742188s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:42:37.818983 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:42:37.818983 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:42:37.819987 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m22:42:48.223623 [debug] [ThreadPool]: SQL status: OK in 10.400 seconds
[0m22:42:48.251212 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=be24eda0-f38d-43bd-a00b-e43d45056443) - Closing
[0m22:42:48.355700 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=11.125759840011597s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:42:48.355700 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=11.125759840011597s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:42:48.356719 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:42:48.356719 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m22:43:01.877194 [debug] [ThreadPool]: SQL status: OK in 13.520 seconds
[0m22:43:01.883929 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=8edcfc8a-5002-42e4-a66f-082aa94d62f9) - Closing
[0m22:43:02.031647 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8284, 28124), compute-name=) - Released connection
[0m22:43:02.033618 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_saleslt, idle-time=0.0029773712158203125s, acquire-count=0, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:43:02.035133 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m22:43:02.036151 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.004492759704589844s, acquire-count=0, language=None, thread-identifier=(8284, 28124), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m22:43:02.039141 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.008500814437866211s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Acquired connection on thread (8284, 28124), using default compute resource for model 'None'
[0m22:43:02.040231 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.009590387344360352s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:43:02.041145 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.009590387344360352s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:43:02.041145 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:43:02.041145 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m22:43:02.259599 [debug] [ThreadPool]: SQL status: OK in 0.220 seconds
[0m22:43:02.261602 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=65b2327e-8af7-4ed3-b510-598fe05aa289) - Closing
[0m22:43:02.264599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.23295855522155762s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:43:02.264599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.23395872116088867s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:43:02.264599 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:43:02.264599 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m22:43:02.544822 [debug] [ThreadPool]: SQL status: OK in 0.280 seconds
[0m22:43:02.548798 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=6ad73dc1-7190-40bd-9b18-0bc82fd1ad1d) - Closing
[0m22:43:02.552793 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.5221519470214844s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Checking idleness
[0m22:43:02.553747 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.523106575012207s, acquire-count=1, language=None, thread-identifier=(8284, 28124), compute-name=) - Retrieving connection
[0m22:43:02.553747 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:43:02.553747 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m22:43:02.876659 [debug] [ThreadPool]: SQL status: OK in 0.320 seconds
[0m22:43:02.878236 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=dd6bc613-49df-4e2e-b433-58905961d735, command-id=5ec61084-5972-4a81-b4b0-83b27549dacb) - Closing
[0m22:43:02.878774 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2673664257200, session-id=dd6bc613-49df-4e2e-b433-58905961d735, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8284, 28124), compute-name=) - Released connection
[0m22:43:02.880941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EEED68370>]}
[0m22:43:02.881445 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=34.844019174575806s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Checking idleness
[0m22:43:02.881999 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=34.844019174575806s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Retrieving connection
[0m22:43:02.881999 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=34.84457302093506s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Checking idleness
[0m22:43:02.882532 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=34.84510564804077s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Retrieving connection
[0m22:43:02.882532 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:43:02.883075 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:43:02.883610 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8284, 9900), compute-name=) - Released connection
[0m22:43:02.895597 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m22:43:02.896688 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m22:43:02.897695 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8284, 12052), compute-name=) - Creating connection
[0m22:43:02.898694 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m22:43:02.898694 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Acquired connection on thread (8284, 12052), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m22:43:02.898694 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m22:43:02.988648 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m22:43:03.265395 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m22:43:03.276564 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m22:43:03.316558 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m22:43:03.386818 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.48812413215637207s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:03.387828 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.4891340732574463s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Retrieving connection
[0m22:43:03.388847 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.49015307426452637s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:03.388847 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.49015307426452637s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Retrieving connection
[0m22:43:03.389818 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:43:03.389818 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m22:43:03.389818 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m22:43:03.390841 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:43:03.562824 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3) - Created
[0m22:43:26.036416 [debug] [Thread-1 (]: SQL status: OK in 22.640 seconds
[0m22:43:26.037496 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, command-id=cec1ceb8-9904-4b43-8e0b-b3a531619edc) - Closing
[0m22:43:26.139050 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:26.140065 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:26.145089 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E82ED3CD0>]}
[0m22:43:26.146096 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 23.24s]
[0m22:43:26.148103 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m22:43:26.148984 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m22:43:26.149986 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m22:43:26.152043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_customer, idle-time=0.010967731475830078s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:26.153049 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m22:43:26.154053 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.012984752655029297s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_customer
[0m22:43:26.155031 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.013988494873046875s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Acquired connection on thread (8284, 12052), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m22:43:26.155031 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m22:43:26.160069 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m22:43:26.250670 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m22:43:26.254656 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m22:43:26.258655 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m22:43:26.316704 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.17663955688476562s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:26.317716 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.17765164375305176s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Retrieving connection
[0m22:43:26.318734 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m22:43:26.318734 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m22:43:32.399230 [debug] [Thread-1 (]: SQL status: OK in 6.080 seconds
[0m22:43:32.402737 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, command-id=c9648887-eb4a-4807-926b-42da97079506) - Closing
[0m22:43:32.474063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:32.475053 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:32.476081 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E828F8EE0>]}
[0m22:43:32.476081 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 6.33s]
[0m22:43:32.477170 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m22:43:32.478087 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.sales
[0m22:43:32.478087 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m22:43:32.479175 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.dim_product, idle-time=0.0041217803955078125s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:32.480086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.sales)
[0m22:43:32.480086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.005032777786254883s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_product
[0m22:43:32.480086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.005032777786254883s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Acquired connection on thread (8284, 12052), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m22:43:32.481092 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.sales
[0m22:43:32.484193 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.sales"
[0m22:43:32.573421 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.sales
[0m22:43:32.578563 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m22:43:32.581567 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.sales"
[0m22:43:32.638523 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.16346955299377441s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Checking idleness
[0m22:43:32.639524 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.16447019577026367s, acquire-count=1, language=sql, thread-identifier=(8284, 12052), compute-name=) - Retrieving connection
[0m22:43:32.640527 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.sales"
[0m22:43:32.640527 [debug] [Thread-1 (]: On model.medallion_dbt_spark.sales: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m22:43:38.915956 [debug] [Thread-1 (]: SQL status: OK in 6.270 seconds
[0m22:43:38.918949 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, command-id=e3c2a354-dd01-4d32-bd56-9a385b243674) - Closing
[0m22:43:38.998831 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:38.999832 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=2673666243840, session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8284, 12052), compute-name=) - Released connection
[0m22:43:39.000832 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9021dde-2aca-4e32-8391-a954cd69b4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E82A0C700>]}
[0m22:43:39.000832 [info ] [Thread-1 (]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 6.52s]
[0m22:43:39.002593 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.sales
[0m22:43:39.006915 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=36.12330508232117s, acquire-count=0, language=None, thread-identifier=(8284, 9900), compute-name=) - Checking idleness
[0m22:43:39.007996 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=36.12438607215881s, acquire-count=0, language=None, thread-identifier=(8284, 9900), compute-name=) - Reusing connection previously named master
[0m22:43:39.007996 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=36.12438607215881s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Acquired connection on thread (8284, 9900), using default compute resource for model 'None'
[0m22:43:39.007996 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=36.12438607215881s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Checking idleness
[0m22:43:39.008999 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=None, name=master, idle-time=36.12538957595825s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Retrieving connection
[0m22:43:39.008999 [debug] [MainThread]: On master: ROLLBACK
[0m22:43:39.008999 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:43:39.183961 [debug] [MainThread]: Databricks adapter: Connection(session-id=43e42649-fa9f-4ca0-9f12-560f11ccb795) - Created
[0m22:43:39.184962 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:43:39.185961 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=43e42649-fa9f-4ca0-9f12-560f11ccb795, name=master, idle-time=0.0009987354278564453s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Checking idleness
[0m22:43:39.186961 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=43e42649-fa9f-4ca0-9f12-560f11ccb795, name=master, idle-time=0.0019986629486083984s, acquire-count=1, language=None, thread-identifier=(8284, 9900), compute-name=) - Retrieving connection
[0m22:43:39.186961 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:43:39.186961 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:43:39.187968 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2673664224720, session-id=43e42649-fa9f-4ca0-9f12-560f11ccb795, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8284, 9900), compute-name=) - Released connection
[0m22:43:39.188969 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:43:39.188969 [debug] [MainThread]: On master: ROLLBACK
[0m22:43:39.189968 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:43:39.189968 [debug] [MainThread]: On master: Close
[0m22:43:39.190976 [debug] [MainThread]: Databricks adapter: Connection(session-id=43e42649-fa9f-4ca0-9f12-560f11ccb795) - Closing
[0m22:43:39.462653 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m22:43:39.463647 [debug] [MainThread]: On list_hive_metastore: Close
[0m22:43:39.464660 [debug] [MainThread]: Databricks adapter: Connection(session-id=ea2c06ec-353d-45b5-a42f-7629e43fb1a1) - Closing
[0m22:43:39.534865 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m22:43:39.536889 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m22:43:39.537867 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:43:39.538861 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m22:43:39.539860 [debug] [MainThread]: Databricks adapter: Connection(session-id=dd6bc613-49df-4e2e-b433-58905961d735) - Closing
[0m22:43:39.608827 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.sales' was properly closed.
[0m22:43:39.609910 [debug] [MainThread]: On model.medallion_dbt_spark.sales: ROLLBACK
[0m22:43:39.610832 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:43:39.611833 [debug] [MainThread]: On model.medallion_dbt_spark.sales: Close
[0m22:43:39.611833 [debug] [MainThread]: Databricks adapter: Connection(session-id=d63a4c72-2c09-4062-9879-e27595fa5fe3) - Closing
[0m22:43:39.676744 [info ] [MainThread]: 
[0m22:43:39.678121 [info ] [MainThread]: Finished running 3 table models in 0 hours 1 minutes and 11.64 seconds (71.64s).
[0m22:43:39.680135 [debug] [MainThread]: Command end result
[0m22:43:39.717043 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:43:39.720038 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:43:39.727486 [debug] [MainThread]: Wrote artifact RunExecutionResult to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\run_results.json
[0m22:43:39.728494 [info ] [MainThread]: 
[0m22:43:39.729531 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:43:39.730481 [info ] [MainThread]: 
[0m22:43:39.731482 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m22:43:39.733486 [debug] [MainThread]: Command `dbt run` succeeded at 22:43:39.733486 after 76.94 seconds
[0m22:43:39.733486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EED22D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E82CB07C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EED2D25C0>]}
[0m22:43:39.734580 [debug] [MainThread]: Flushing usage events
[0m22:43:40.121819 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:44:16.579557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023946F8D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002394866C7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002394866E2F0>]}


============================== 22:44:16.584797 | 0d8238ae-75ad-493a-be1c-08deb8e0b443 ==============================
[0m22:44:16.584797 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:44:16.585707 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:44:17.231861 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:44:17.232862 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:44:17.232862 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:44:18.318715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0d8238ae-75ad-493a-be1c-08deb8e0b443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002395A292E00>]}
[0m22:44:18.394910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0d8238ae-75ad-493a-be1c-08deb8e0b443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023949CADFC0>]}
[0m22:44:18.396420 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m22:44:18.795654 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:44:19.060336 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:44:19.060336 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:44:19.137485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0d8238ae-75ad-493a-be1c-08deb8e0b443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002395B8C9270>]}
[0m22:44:19.299823 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:44:19.304825 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:44:19.421011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0d8238ae-75ad-493a-be1c-08deb8e0b443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002395B784F10>]}
[0m22:44:19.421011 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 607 macros
[0m22:44:19.422095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0d8238ae-75ad-493a-be1c-08deb8e0b443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002395B785150>]}
[0m22:44:19.424011 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:44:19.426513 [debug] [MainThread]: Command end result
[0m22:44:19.466532 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:44:19.469539 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:44:19.476469 [debug] [MainThread]: Wrote artifact RunExecutionResult to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\run_results.json
[0m22:44:19.477654 [debug] [MainThread]: Command `dbt test` succeeded at 22:44:19.477654 after 3.00 seconds
[0m22:44:19.478663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023946F8D480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023948AC8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002394863F1C0>]}
[0m22:44:19.478663 [debug] [MainThread]: Flushing usage events
[0m22:44:19.857603 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:41.903358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE4131420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE5EA4EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE5EA61D0>]}


============================== 22:46:41.907578 | 8543679d-d8d0-4167-8d3a-72c39dc5fc12 ==============================
[0m22:46:41.907578 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:46:41.908649 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt docs generate', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:46:42.547771 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:46:42.548772 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:46:42.548772 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:46:43.572296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE25E1090>]}
[0m22:46:43.645536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE5E5E890>]}
[0m22:46:43.646635 [info ] [MainThread]: Registered adapter: databricks=1.9.7
[0m22:46:44.021554 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:46:44.261592 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:46:44.261592 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:46:44.330275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EF8A691B0>]}
[0m22:46:44.370111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EF8A08FD0>]}
[0m22:46:44.371110 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 607 macros
[0m22:46:44.372107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EF8A08FA0>]}
[0m22:46:44.374720 [info ] [MainThread]: 
[0m22:46:44.375239 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:46:44.375239 [info ] [MainThread]: 
[0m22:46:44.376448 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576129301760, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(3440, 8768), compute-name=) - Creating connection
[0m22:46:44.376448 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:46:44.377454 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576129301760, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(3440, 8768), compute-name=) - Acquired connection on thread (3440, 8768), using default compute resource for model 'None'
[0m22:46:44.384456 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, acquire-count=0, language=None, thread-identifier=(3440, 21604), compute-name=) - Creating connection
[0m22:46:44.385614 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m22:46:44.386719 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Acquired connection on thread (3440, 21604), using default compute resource for model 'None'
[0m22:46:44.387283 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0005631446838378906s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:44.387283 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=None, name=list_hive_metastore_snapshots, idle-time=0.0005631446838378906s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:44.387283 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:46:44.387283 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m22:46:44.388374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:44.613458 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2) - Created
[0m22:46:45.259406 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m22:46:45.260432 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=fd72640c-29bc-4eb0-aaca-25f222ee7d50) - Closing
[0m22:46:45.274821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.6602847576141357s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.274821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.6602847576141357s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:45.275444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.6609079837799072s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.275444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.6609079837799072s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:45.275444 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:46:45.276488 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:46:45.276488 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m22:46:45.463139 [debug] [ThreadPool]: SQL status: OK in 0.190 seconds
[0m22:46:45.470153 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=947daba6-e1c6-4ffe-9098-c950a4fcb6f3) - Closing
[0m22:46:45.480189 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.8646488189697266s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.481185 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.8656530380249023s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:45.481185 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m22:46:45.482185 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m22:46:45.772464 [debug] [ThreadPool]: SQL status: OK in 0.290 seconds
[0m22:46:45.777975 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=0f610cfe-880b-4de1-baf0-ac4ebd8de8ad) - Closing
[0m22:46:45.780093 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 21604), compute-name=) - Released connection
[0m22:46:45.780986 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_snapshots, idle-time=0.0019981861114501953s, acquire-count=0, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.785528 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m22:46:45.786624 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.0076367855072021484s, acquire-count=0, language=None, thread-identifier=(3440, 21604), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m22:46:45.787623 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.008635759353637695s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Acquired connection on thread (3440, 21604), using default compute resource for model 'None'
[0m22:46:45.788629 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.008635759353637695s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.788629 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.009641647338867188s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:45.789628 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:46:45.789628 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt)
[0m22:46:45.943549 [debug] [ThreadPool]: SQL status: OK in 0.150 seconds
[0m22:46:45.944649 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=48b8be72-6d43-4139-bd33-56efacab451d) - Closing
[0m22:46:45.947674 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.1686861515045166s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:45.947674 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.1686861515045166s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:45.948654 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:46:45.948654 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m22:46:46.099268 [debug] [ThreadPool]: SQL status: OK in 0.150 seconds
[0m22:46:46.102267 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=045f67f1-1e70-450e-b9cd-f888514064b2) - Closing
[0m22:46:46.105296 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.3263087272644043s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Checking idleness
[0m22:46:46.105296 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.3263087272644043s, acquire-count=1, language=None, thread-identifier=(3440, 21604), compute-name=) - Retrieving connection
[0m22:46:46.106306 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m22:46:46.106306 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m22:46:46.380707 [debug] [ThreadPool]: SQL status: OK in 0.270 seconds
[0m22:46:46.383283 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, command-id=dc6990d7-ae04-4b07-815e-294bde30c78e) - Closing
[0m22:46:46.384791 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576129693392, session-id=512bdfb3-a507-494a-809f-28b64c9dcad2, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 21604), compute-name=) - Released connection
[0m22:46:46.387403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8543679d-d8d0-4167-8d3a-72c39dc5fc12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EF8A0B5E0>]}
[0m22:46:46.387403 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576129301760, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 8768), compute-name=) - Released connection
[0m22:46:46.393443 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m22:46:46.394403 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(3440, 24316), compute-name=) - Creating connection
[0m22:46:46.394907 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m22:46:46.394907 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m22:46:46.395410 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m22:46:46.404422 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m22:46:46.404422 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.405467 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.406555 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m22:46:46.406555 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:46:46.406555 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0010879039764404297s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.407573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m22:46:46.407573 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.002105236053466797s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.address_snapshot
[0m22:46:46.407573 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.002105236053466797s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m22:46:46.408592 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:46:46.411571 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:46:46.412489 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.412489 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.413487 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:46:46.413487 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:46:46.413487 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0009980201721191406s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.414578 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m22:46:46.415190 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0020885467529296875s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customer_snapshot
[0m22:46:46.415190 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0027010440826416016s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m22:46:46.415190 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:46:46.419765 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:46:46.420766 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.420766 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.421885 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:46:46.421885 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m22:46:46.422862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.002095460891723633s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.422862 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m22:46:46.422862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.002095460891723633s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:46:46.422862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.002095460891723633s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m22:46:46.424410 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m22:46:46.428223 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m22:46:46.429128 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.429128 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.429128 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m22:46:46.430126 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:46:46.430126 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0009984970092773438s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.431207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m22:46:46.431207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.002079010009765625s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.product_snapshot
[0m22:46:46.431207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.002079010009765625s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m22:46:46.432206 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:46:46.500288 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:46:46.501290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.502288 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.502288 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:46:46.503299 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:46:46.503299 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0020084381103515625s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.503299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m22:46:46.505365 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.003571033477783203s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:46:46.505365 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.004075050354003906s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m22:46:46.505365 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:46:46.509375 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:46:46.509375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.510374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.510374 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:46:46.511375 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:46:46.511375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0010001659393310547s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.511375 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m22:46:46.512375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0020008087158203125s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:46:46.512375 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0020008087158203125s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m22:46:46.513379 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:46:46.516385 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:46:46.517390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.517390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.518390 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:46:46.519404 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m22:46:46.519404 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.00201416015625s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.520390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderheader_snapshot, now model.medallion_dbt_spark.dim_customer)
[0m22:46:46.520390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0029993057250976562s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:46:46.521502 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.004111766815185547s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m22:46:46.521502 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m22:46:46.525697 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m22:46:46.526800 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m22:46:46.527815 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.528813 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.528813 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m22:46:46.529913 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m22:46:46.529913 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0010998249053955078s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.530917 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m22:46:46.530917 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.002104520797729492s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_customer
[0m22:46:46.530917 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.002104520797729492s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m22:46:46.531893 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m22:46:46.536075 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m22:46:46.538091 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m22:46:46.539089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.540093 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.540093 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m22:46:46.541089 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.sales
[0m22:46:46.541089 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.0009965896606445312s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Checking idleness
[0m22:46:46.542084 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.sales)
[0m22:46:46.542084 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.00199127197265625s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_product
[0m22:46:46.543084 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.00199127197265625s, acquire-count=1, language=sql, thread-identifier=(3440, 24316), compute-name=) - Acquired connection on thread (3440, 24316), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m22:46:46.543084 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.sales
[0m22:46:46.548212 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.sales"
[0m22:46:46.549212 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.sales
[0m22:46:46.549212 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.550213 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=1576106396016, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(3440, 24316), compute-name=) - Released connection
[0m22:46:46.551219 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.sales
[0m22:46:46.553209 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:46:46.553209 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m22:46:46.553209 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m22:46:46.554817 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:46:46.554817 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m22:46:46.555441 [debug] [MainThread]: Databricks adapter: Connection(session-id=512bdfb3-a507-494a-809f-28b64c9dcad2) - Closing
[0m22:46:46.639403 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.sales' was properly closed.
[0m22:46:46.641406 [debug] [MainThread]: Command end result
[0m22:46:46.760529 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:46:46.763536 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:46:46.771925 [debug] [MainThread]: Wrote artifact RunExecutionResult to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\run_results.json
[0m22:46:47.158766 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576130730240, session-id=None, name=generate_catalog, idle-time=0s, acquire-count=0, language=None, thread-identifier=(3440, 8768), compute-name=) - Creating connection
[0m22:46:47.159766 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m22:46:47.159766 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576130730240, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(3440, 8768), compute-name=) - Acquired connection on thread (3440, 8768), using default compute resource for model 'None'
[0m22:46:47.159766 [info ] [MainThread]: Building catalog
[0m22:46:47.163385 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0s, acquire-count=0, language=None, thread-identifier=(3440, 24392), compute-name=) - Creating connection
[0m22:46:47.163385 [debug] [ThreadPool]: Acquiring new databricks connection '('hive_metastore', 'saleslt')'
[0m22:46:47.164337 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Acquired connection on thread (3440, 24392), using default compute resource for model 'None'
[0m22:46:47.165540 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0012028217315673828s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:47.167175 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0012028217315673828s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Retrieving connection
[0m22:46:47.167244 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0029070377349853516s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:47.167244 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0029070377349853516s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Retrieving connection
[0m22:46:47.167244 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:46:47.168221 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m22:46:47.168221 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */

      select current_catalog()
  
[0m22:46:47.168221 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:47.357209 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf) - Created
[0m22:46:47.497773 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m22:46:47.500781 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, command-id=346d5d7a-eb67-4067-818f-f47e48487ebd) - Closing
[0m22:46:47.507553 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'saleslt'), idle-time=0.1503443717956543s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:47.507553 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'saleslt'), idle-time=0.1503443717956543s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Retrieving connection
[0m22:46:47.507553 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m22:46:47.508528 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */
show table extended in `hive_metastore`.`saleslt` like 'dim_product|productmodel|sales|customer|productdescription|customeraddress|salesorderheader|address|product|productcategory|dim_customer|salesorderdetail'
  
[0m22:46:48.549876 [debug] [ThreadPool]: SQL status: OK in 1.040 seconds
[0m22:46:48.559536 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, command-id=a7f5a81e-767d-481e-99a2-1f143ca18a84) - Closing
[0m22:46:48.569640 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 24392), compute-name=) - Released connection
[0m22:46:48.570145 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'saleslt'), idle-time=0.0005049705505371094s, acquire-count=0, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:48.571150 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly ('hive_metastore', 'saleslt'), now ('hive_metastore', 'snapshots'))
[0m22:46:48.571150 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.0015094280242919922s, acquire-count=0, language=None, thread-identifier=(3440, 24392), compute-name=) - Reusing connection previously named ('hive_metastore', 'saleslt')
[0m22:46:48.571150 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.0015094280242919922s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Acquired connection on thread (3440, 24392), using default compute resource for model 'None'
[0m22:46:48.574728 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.00401616096496582s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:48.574728 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.005087852478027344s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Retrieving connection
[0m22:46:48.575335 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m22:46:48.575335 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */

      select current_catalog()
  
[0m22:46:48.694344 [debug] [ThreadPool]: SQL status: OK in 0.120 seconds
[0m22:46:48.697370 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, command-id=ee09ead8-23bb-41db-b169-7ea930429f22) - Closing
[0m22:46:48.700368 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.13072776794433594s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Checking idleness
[0m22:46:48.700368 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.13072776794433594s, acquire-count=1, language=None, thread-identifier=(3440, 24392), compute-name=) - Retrieving connection
[0m22:46:48.700368 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m22:46:48.701369 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.9.7", "databricks_sql_connector_version": "3.7.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */
show table extended in `hive_metastore`.`snapshots` like 'productmodel_snapshot|customeraddress_snapshot|product_snapshot|salesorderheader_snapshot|customer_snapshot|salesorderdetail_snapshot|address_snapshot'
  
[0m22:46:49.667683 [debug] [ThreadPool]: SQL status: OK in 0.970 seconds
[0m22:46:49.670107 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, command-id=610f9add-d434-4aab-b62f-ad18672bb2fe) - Closing
[0m22:46:49.673662 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1576130896720, session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf, name=('hive_metastore', 'snapshots'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 24392), compute-name=) - Released connection
[0m22:46:49.677477 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1576130730240, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(3440, 8768), compute-name=) - Released connection
[0m22:46:49.691599 [debug] [MainThread]: Wrote artifact CatalogArtifact to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\catalog.json
[0m22:46:49.725563 [debug] [MainThread]: Wrote artifact WritableManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\manifest.json
[0m22:46:49.728903 [debug] [MainThread]: Wrote artifact SemanticManifest to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\semantic_manifest.json
[0m22:46:49.728903 [info ] [MainThread]: Catalog written to Z:\cours_quatri√®me_ann√©e\PersonalCourse\DataEngineeringProjectAzure\Medallion-Spark-Azure-DBT\virtualEnv\medallion_dbt_spark\DataEngineerProjectWithAzure\target\catalog.json
[0m22:46:49.730989 [debug] [MainThread]: Command `dbt docs generate` succeeded at 22:46:49.730989 after 8.05 seconds
[0m22:46:49.730989 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m22:46:49.731996 [debug] [MainThread]: Connection '('hive_metastore', 'snapshots')' was properly closed.
[0m22:46:49.731996 [debug] [MainThread]: On ('hive_metastore', 'snapshots'): ROLLBACK
[0m22:46:49.731996 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:46:49.732997 [debug] [MainThread]: On ('hive_metastore', 'snapshots'): Close
[0m22:46:49.732997 [debug] [MainThread]: Databricks adapter: Connection(session-id=c34337d2-da69-418d-8dda-4c3e0f3f02cf) - Closing
[0m22:46:49.797314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE4131420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EE582DE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EF74905B0>]}
[0m22:46:49.797314 [debug] [MainThread]: Flushing usage events
[0m22:46:50.179580 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:47:04.059687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1DA2E13F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1DC052530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1DC053F10>]}


============================== 22:47:04.067365 | b88999d6-d0bd-412b-b3b3-2a0e7d335da7 ==============================
[0m22:47:04.067365 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:47:04.068327 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\mlkou\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'Z:\\cours_quatri√®me_ann√©e\\PersonalCourse\\DataEngineeringProjectAzure\\Medallion-Spark-Azure-DBT\\virtualEnv\\medallion_dbt_spark\\DataEngineerProjectWithAzure\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt docs serve', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m22:47:04.709303 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:47:04.709303 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:47:04.710304 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:47:05.728446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b88999d6-d0bd-412b-b3b3-2a0e7d335da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1D875CFD0>]}
[0m22:47:05.801847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b88999d6-d0bd-412b-b3b3-2a0e7d335da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1ED2CF700>]}
[0m22:53:40.377128 [error] [MainThread]: Encountered an error:

[0m22:53:40.428536 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\requires.py", line 235, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\requires.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\cli\main.py", line 301, in docs_serve
    results = task.run()
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\site-packages\dbt\task\docs\serve.py", line 29, in run
    httpd.serve_forever()
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\socketserver.py", line 232, in serve_forever
    ready = selector.select(poll_interval)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\selectors.py", line 324, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
  File "C:\Users\mlkou\AppData\Local\Programs\Python\Python310\lib\selectors.py", line 315, in _select
    r, w, x = select.select(r, w, w, timeout)
KeyboardInterrupt

[0m22:53:40.432590 [debug] [MainThread]: Command `dbt docs serve` failed at 22:53:40.431542 after 396.48 seconds
[0m22:53:40.432590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1DA2E13F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1ED78D240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1ED78D330>]}
[0m22:53:40.432590 [debug] [MainThread]: Flushing usage events
[0m22:53:40.820071 [debug] [MainThread]: An error was encountered while trying to flush usage events
